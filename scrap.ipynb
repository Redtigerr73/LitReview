{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleScholarSearch\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from credentials import API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_json_file(file_path:str, data:dict):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            existing_data = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = []\n",
    "\n",
    "    existing_data.extend(data)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(existing_data, file, indent=4)\n",
    "\n",
    "def query_google_scholar(query):\n",
    "    # Set up SerpApi parameters\n",
    "    api_key = None  # Replace with your SerpApi API key\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'api_key': api_key,\n",
    "        'as_ylo': 2019,  # Filter results from 2019 onwards\n",
    "        'page_size': 20,  # Set the page size to 20\n",
    "        'as_rr': 0,  # If 1, filter only review papers\n",
    "    }\n",
    "\n",
    "    # Perform the search\n",
    "    search = GoogleScholarSearch(params)\n",
    "\n",
    "    # Save all results to a JSON file\n",
    "    save_path = 'google_scholar_results.json'\n",
    "    \n",
    "    while True:\n",
    "        results = search.get_dict()\n",
    "\n",
    "        if results.get('search_metadata').get('status') != 'Success':\n",
    "            print(f\"ERROR.{results}\")\n",
    "            break\n",
    "\n",
    "        if results:\n",
    "            append_to_json_file(save_path, results)\n",
    "\n",
    "        if not search.pagination():\n",
    "            break\n",
    "\n",
    "    print(f\"All results saved to {save_path}\")\n",
    "    if search.num_pages():\n",
    "        print(f\"All pages extracted: {search.num_pages()}\")\n",
    "\n",
    "# Example usage:\n",
    "query_string = 'multiple sclerosis segmentation deep learning'\n",
    "query_google_scholar(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_append_to_csv(result_list, csv_filename='extracted_info.csv'):\n",
    "    # Check if the CSV file exists\n",
    "    file_exists = os.path.exists(csv_filename)\n",
    "\n",
    "    # Save extracted information to a CSV file (append mode)\n",
    "    fieldnames = ['title', 'link', 'citation', 'summary', 'resource_title', 'file_format']\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write header only if the file is empty\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Iterate through the list of results\n",
    "        for result in result_list:\n",
    "            extracted_info = []\n",
    "\n",
    "            for item in result.get('organic_results', []):\n",
    "                info = {}\n",
    "\n",
    "                # Check if the fields exist before extracting information\n",
    "                title = item.get('title')\n",
    "                link = item.get('link')\n",
    "                citation = item.get('inline_links', {}).get('cited_by', {}).get('total')\n",
    "                summary = item.get('publication_info', {}).get('summary')\n",
    "\n",
    "                resources = item.get('resources', [])\n",
    "                if resources:\n",
    "                    resource = resources[0]  # Assuming you want information from the first resource\n",
    "                    resource_title = resource.get('title')\n",
    "                    file_format = resource.get('file_format')\n",
    "                else:\n",
    "                    resource_title = None\n",
    "                    file_format = None\n",
    "\n",
    "                # Add information to the extracted_info list\n",
    "                info['title'] = title\n",
    "                info['link'] = link\n",
    "                info['citation'] = citation\n",
    "                info['summary'] = summary\n",
    "                info['resource_title'] = resource_title\n",
    "                info['file_format'] = file_format\n",
    "\n",
    "                extracted_info.append(info)\n",
    "\n",
    "            # Write data rows\n",
    "            writer.writerows(extracted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"articles_review.json\", \"r\") as f:\n",
    "    search_results = json.load(f)\n",
    "extract_and_append_to_csv(search_results, \"article_review.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LitReview-BPA3kgP-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
